{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":["import sys\n","# sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"],"execution_count":1,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":["import math, random\n","import gc, os\n","import datetime\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from sklearn.kernel_approximation import Nystroem\n","from sklearn.decomposition import PCA, KernelPCA, TruncatedSVD\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import log_loss\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.cluster import FeatureAgglomeration, AgglomerativeClustering, KMeans\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA \n","from tqdm.notebook import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","# from hyperopt import hp, fmin, tpe, Trials\n","# from hyperopt.pyll.base import scope\n","from tqdm.notebook import tqdm\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":2,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":["train_features = pd.read_csv(\"../input/lish-moa/train_features.csv\")\n","train_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n","train_nontargets = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n","\n","test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n","ss = pd.read_csv('../input/lish-moa/sample_submission.csv')"],"execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def preprocess(df):\n","    df = df.copy()\n","    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n","    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n","    del df['sig_id']\n","    return df\n","\n","train = preprocess(train_features)\n","test = preprocess(test_features)\n","\n","del train_targets['sig_id']\n","\n","train_targets = train_targets.loc[train['cp_type']==0].reset_index(drop=True)\n","train = train.loc[train['cp_type']==0].reset_index(drop=True)"],"execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def seed_everything(seed_value):\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    os.environ['PYTHONHASHSEED'] = str(seed_value)\n","    \n","    if torch.cuda.is_available(): \n","        torch.cuda.manual_seed(seed_value)\n","        torch.cuda.manual_seed_all(seed_value)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","        \n","seed_everything(42)"],"execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["nfolds = 10\n","nstarts = 1\n","nepochs = 200\n","batch_size = 128\n","# batch_size = 1024\n","val_batch_size = batch_size * 4\n","ntargets = train_targets.shape[1]\n","targets = [col for col in train_targets.columns]\n","criterion = nn.BCELoss()\n","\n","# for GPU/CPU\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# ODST/NODE\n","\n","https://github.com/Qwicen/node"]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":["import contextlib\n","from torch.autograd import Function\n","from collections import OrderedDict\n","from torch.jit import script\n","from warnings import warn\n","import os\n","import glob\n","import hashlib\n","import gc\n","import time\n","import requests\n","\n","def to_one_hot(y, depth=None):\n","    r\"\"\"\n","    Takes integer with n dims and converts it to 1-hot representation with n + 1 dims.\n","    The n+1'st dimension will have zeros everywhere but at y'th index, where it will be equal to 1.\n","    Args:\n","        y: input integer (IntTensor, LongTensor or Variable) of any shape\n","        depth (int):  the size of the one hot dimension\n","    \"\"\"\n","    y_flat = y.to(torch.int64).view(-1, 1)\n","    depth = depth if depth is not None else int(torch.max(y_flat)) + 1\n","    y_one_hot = torch.zeros(y_flat.size()[0], depth, device=y.device).scatter_(1, y_flat, 1)\n","    y_one_hot = y_one_hot.view(*(tuple(y.shape) + (-1,)))\n","    return y_one_hot\n","\n","\n","def _make_ix_like(input, dim=0):\n","    d = input.size(dim)\n","    rho = torch.arange(1, d + 1, device=input.device, dtype=input.dtype)\n","    view = [1] * input.dim()\n","    view[0] = -1\n","    return rho.view(view).transpose(0, dim)\n","\n","\n","class SparsemaxFunction(Function):\n","    \"\"\"\n","    An implementation of sparsemax (Martins & Astudillo, 2016). See\n","    :cite:`DBLP:journals/corr/MartinsA16` for detailed description.\n","    By Ben Peters and Vlad Niculae\n","    \"\"\"\n","\n","    @staticmethod\n","    def forward(ctx, input, dim=-1):\n","        \"\"\"sparsemax: normalizing sparse transform (a la softmax)\n","        Parameters:\n","            input (Tensor): any shape\n","            dim: dimension along which to apply sparsemax\n","        Returns:\n","            output (Tensor): same shape as input\n","        \"\"\"\n","        ctx.dim = dim\n","        max_val, _ = input.max(dim=dim, keepdim=True)\n","        input -= max_val  # same numerical stability trick as for softmax\n","        tau, supp_size = SparsemaxFunction._threshold_and_support(input, dim=dim)\n","        output = torch.clamp(input - tau, min=0)\n","        ctx.save_for_backward(supp_size, output)\n","        return output\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        supp_size, output = ctx.saved_tensors\n","        dim = ctx.dim\n","        grad_input = grad_output.clone()\n","        grad_input[output == 0] = 0\n","\n","        v_hat = grad_input.sum(dim=dim) / supp_size.to(output.dtype).squeeze()\n","        v_hat = v_hat.unsqueeze(dim)\n","        grad_input = torch.where(output != 0, grad_input - v_hat, grad_input)\n","        return grad_input, None\n","\n","\n","    @staticmethod\n","    def _threshold_and_support(input, dim=-1):\n","        \"\"\"Sparsemax building block: compute the threshold\n","        Args:\n","            input: any dimension\n","            dim: dimension along which to apply the sparsemax\n","        Returns:\n","            the threshold value\n","        \"\"\"\n","\n","        input_srt, _ = torch.sort(input, descending=True, dim=dim)\n","        input_cumsum = input_srt.cumsum(dim) - 1\n","        rhos = _make_ix_like(input, dim)\n","        support = rhos * input_srt > input_cumsum\n","\n","        support_size = support.sum(dim=dim).unsqueeze(dim)\n","        tau = input_cumsum.gather(dim, support_size - 1)\n","        tau /= support_size.to(input.dtype)\n","        return tau, support_size\n","\n","\n","sparsemax = lambda input, dim=-1: SparsemaxFunction.apply(input, dim)\n","sparsemoid = lambda input: (0.5 * input + 0.5).clamp_(0, 1)\n","\n","\n","class Entmax15Function(Function):\n","    \"\"\"\n","    An implementation of exact Entmax with alpha=1.5 (B. Peters, V. Niculae, A. Martins). See\n","    :cite:`https://arxiv.org/abs/1905.05702 for detailed description.\n","    Source: https://github.com/deep-spin/entmax\n","    \"\"\"\n","\n","    @staticmethod\n","    def forward(ctx, input, dim=-1):\n","        ctx.dim = dim\n","\n","        max_val, _ = input.max(dim=dim, keepdim=True)\n","        input = input - max_val  # same numerical stability trick as for softmax\n","        input = input / 2  # divide by 2 to solve actual Entmax\n","\n","        tau_star, _ = Entmax15Function._threshold_and_support(input, dim)\n","        output = torch.clamp(input - tau_star, min=0) ** 2\n","        ctx.save_for_backward(output)\n","        return output\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        Y, = ctx.saved_tensors\n","        gppr = Y.sqrt()  # = 1 / g'' (Y)\n","        dX = grad_output * gppr\n","        q = dX.sum(ctx.dim) / gppr.sum(ctx.dim)\n","        q = q.unsqueeze(ctx.dim)\n","        dX -= q * gppr\n","        return dX, None\n","\n","    @staticmethod\n","    def _threshold_and_support(input, dim=-1):\n","        Xsrt, _ = torch.sort(input, descending=True, dim=dim)\n","\n","        rho = _make_ix_like(input, dim)\n","        mean = Xsrt.cumsum(dim) / rho\n","        mean_sq = (Xsrt ** 2).cumsum(dim) / rho\n","        ss = rho * (mean_sq - mean ** 2)\n","        delta = (1 - ss) / rho\n","\n","        # NOTE this is not exactly the same as in reference algo\n","        # Fortunately it seems the clamped values never wrongly\n","        # get selected by tau <= sorted_z. Prove this!\n","        delta_nz = torch.clamp(delta, 0)\n","        tau = mean - torch.sqrt(delta_nz)\n","\n","        support_size = (tau <= Xsrt).sum(dim).unsqueeze(dim)\n","        tau_star = tau.gather(dim, support_size - 1)\n","        return tau_star, support_size\n","\n","\n","class Entmoid15(Function):\n","    \"\"\" A highly optimized equivalent of labda x: Entmax15([x, 0]) \"\"\"\n","\n","    @staticmethod\n","    def forward(ctx, input):\n","        output = Entmoid15._forward(input)\n","        ctx.save_for_backward(output)\n","        return output\n","\n","    @staticmethod\n","    @script\n","    def _forward(input):\n","        input, is_pos = abs(input), input >= 0\n","        tau = (input + torch.sqrt(F.relu(8 - input ** 2))) / 2\n","        tau.masked_fill_(tau <= input, 2.0)\n","        y_neg = 0.25 * F.relu(tau - input, inplace=True) ** 2\n","        return torch.where(is_pos, 1 - y_neg, y_neg)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        return Entmoid15._backward(ctx.saved_tensors[0], grad_output)\n","\n","    @staticmethod\n","    @script\n","    def _backward(output, grad_output):\n","        gppr0, gppr1 = output.sqrt(), (1 - output).sqrt()\n","        grad_input = grad_output * gppr0\n","        q = grad_input / (gppr0 + gppr1)\n","        grad_input -= q * gppr0\n","        return grad_input\n","\n","\n","entmax15 = lambda input, dim=-1: Entmax15Function.apply(input, dim)\n","entmoid15 = Entmoid15.apply\n","\n","\n","class Lambda(nn.Module):\n","    def __init__(self, func):\n","        super().__init__()\n","        self.func = func\n","\n","    def forward(self, *args, **kwargs):\n","        return self.func(*args, **kwargs)\n","\n","\n","class ModuleWithInit(nn.Module):\n","    \"\"\" Base class for pytorch module with data-aware initializer on first batch \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","        self._is_initialized_tensor = nn.Parameter(torch.tensor(0, dtype=torch.uint8), requires_grad=False)\n","        self._is_initialized_bool = None\n","        # Note: this module uses a separate flag self._is_initialized so as to achieve both\n","        # * persistence: is_initialized is saved alongside model in state_dict\n","        # * speed: model doesn't need to cache\n","        # please DO NOT use these flags in child modules\n","\n","    def initialize(self, *args, **kwargs):\n","        \"\"\" initialize module tensors using first batch of data \"\"\"\n","        raise NotImplementedError(\"Please implement \")\n","\n","    def __call__(self, *args, **kwargs):\n","        if self._is_initialized_bool is None:\n","            self._is_initialized_bool = bool(self._is_initialized_tensor.item())\n","        if not self._is_initialized_bool:\n","            self.initialize(*args, **kwargs)\n","            self._is_initialized_tensor.data[...] = 1\n","            self._is_initialized_bool = True\n","        return super().__call__(*args, **kwargs)\n","\n","def download(url, filename, delete_if_interrupted=True, chunk_size=4096):\n","    \"\"\" saves file from url to filename with a fancy progressbar \"\"\"\n","    try:\n","        with open(filename, \"wb\") as f:\n","            print(\"Downloading {} > {}\".format(url, filename))\n","            response = requests.get(url, stream=True)\n","            total_length = response.headers.get('content-length')\n","\n","            if total_length is None:  # no content length header\n","                f.write(response.content)\n","            else:\n","                total_length = int(total_length)\n","                with tqdm(total=total_length) as progressbar:\n","                    for data in response.iter_content(chunk_size=chunk_size):\n","                        if data:  # filter-out keep-alive chunks\n","                            f.write(data)\n","                            progressbar.update(len(data))\n","    except Exception as e:\n","        if delete_if_interrupted:\n","            print(\"Removing incomplete download {}.\".format(filename))\n","            os.remove(filename)\n","        raise e\n","    return filename\n","\n","\n","def iterate_minibatches(*tensors, batch_size, shuffle=True, epochs=1,\n","                        allow_incomplete=True, callback=lambda x:x):\n","    indices = np.arange(len(tensors[0]))\n","    upper_bound = int((np.ceil if allow_incomplete else np.floor) (len(indices) / batch_size)) * batch_size\n","    epoch = 0\n","    while True:\n","        if shuffle:\n","            np.random.shuffle(indices)\n","        for batch_start in callback(range(0, upper_bound, batch_size)):\n","            batch_ix = indices[batch_start: batch_start + batch_size]\n","            batch = [tensor[batch_ix] for tensor in tensors]\n","            yield batch if len(tensors) > 1 else batch[0]\n","        epoch += 1\n","        if epoch >= epochs:\n","            break\n","\n","\n","def process_in_chunks(function, *args, batch_size, out=None, **kwargs):\n","    \"\"\"\n","    Computes output by applying batch-parallel function to large data tensor in chunks\n","    :param function: a function(*[x[indices, ...] for x in args]) -> out[indices, ...]\n","    :param args: one or many tensors, each [num_instances, ...]\n","    :param batch_size: maximum chunk size processed in one go\n","    :param out: memory buffer for out, defaults to torch.zeros of appropriate size and type\n","    :returns: function(data), computed in a memory-efficient way\n","    \"\"\"\n","    total_size = args[0].shape[0]\n","    first_output = function(*[x[0: batch_size] for x in args])\n","    output_shape = (total_size,) + tuple(first_output.shape[1:])\n","    if out is None:\n","        out = torch.zeros(*output_shape, dtype=first_output.dtype, device=first_output.device,\n","                          layout=first_output.layout, **kwargs)\n","\n","    out[0: batch_size] = first_output\n","    for i in range(batch_size, total_size, batch_size):\n","        batch_ix = slice(i, min(i + batch_size, total_size))\n","        out[batch_ix] = function(*[x[batch_ix] for x in args])\n","    return out\n","\n","\n","def check_numpy(x):\n","    \"\"\" Makes sure x is a numpy array \"\"\"\n","    if isinstance(x, torch.Tensor):\n","        x = x.detach().cpu().numpy()\n","    x = np.asarray(x)\n","    assert isinstance(x, np.ndarray)\n","    return x\n","\n","\n","@contextlib.contextmanager\n","def nop_ctx():\n","    yield None\n","\n","\n","def get_latest_file(pattern):\n","    list_of_files = glob.glob(pattern) # * means all if need specific format then *.csv\n","    assert len(list_of_files) > 0, \"No files found: \" + pattern\n","    return max(list_of_files, key=os.path.getctime)\n","\n","\n","def md5sum(fname):\n","    \"\"\" Computes mdp checksum of a file \"\"\"\n","    hash_md5 = hashlib.md5()\n","    with open(fname, \"rb\") as f:\n","        for chunk in iter(lambda: f.read(4096), b\"\"):\n","            hash_md5.update(chunk)\n","    return hash_md5.hexdigest()\n","\n","\n","def free_memory(sleep_time=0.1):\n","    \"\"\" Black magic function to free torch memory and some jupyter whims \"\"\"\n","    gc.collect()\n","    torch.cuda.synchronize()\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    time.sleep(sleep_time)\n","\n","def to_float_str(element):\n","    try:\n","        return str(float(element))\n","    except ValueError:\n","        return element\n","    \n","class ODST(ModuleWithInit):\n","    def __init__(self, in_features, num_trees, depth=6, tree_dim=1, flatten_output=True,\n","                 choice_function=sparsemax, bin_function=sparsemoid,\n","                 initialize_response_=nn.init.normal_, initialize_selection_logits_=nn.init.uniform_,\n","                 threshold_init_beta=1.0, threshold_init_cutoff=1.0,\n","                 ):\n","        \"\"\"\n","        Oblivious Differentiable Sparsemax Trees. http://tinyurl.com/odst-readmore\n","        One can drop (sic!) this module anywhere instead of nn.Linear\n","        :param in_features: number of features in the input tensor\n","        :param num_trees: number of trees in this layer\n","        :param tree_dim: number of response channels in the response of individual tree\n","        :param depth: number of splits in every tree\n","        :param flatten_output: if False, returns [..., num_trees, tree_dim],\n","            by default returns [..., num_trees * tree_dim]\n","        :param choice_function: f(tensor, dim) -> R_simplex computes feature weights s.t. f(tensor, dim).sum(dim) == 1\n","        :param bin_function: f(tensor) -> R[0, 1], computes tree leaf weights\n","        :param initialize_response_: in-place initializer for tree output tensor\n","        :param initialize_selection_logits_: in-place initializer for logits that select features for the tree\n","        both thresholds and scales are initialized with data-aware init (or .load_state_dict)\n","        :param threshold_init_beta: initializes threshold to a q-th quantile of data points\n","            where q ~ Beta(:threshold_init_beta:, :threshold_init_beta:)\n","            If this param is set to 1, initial thresholds will have the same distribution as data points\n","            If greater than 1 (e.g. 10), thresholds will be closer to median data value\n","            If less than 1 (e.g. 0.1), thresholds will approach min/max data values.\n","        :param threshold_init_cutoff: threshold log-temperatures initializer, \\in (0, inf)\n","            By default(1.0), log-remperatures are initialized in such a way that all bin selectors\n","            end up in the linear region of sparse-sigmoid. The temperatures are then scaled by this parameter.\n","            Setting this value > 1.0 will result in some margin between data points and sparse-sigmoid cutoff value\n","            Setting this value < 1.0 will cause (1 - value) part of data points to end up in flat sparse-sigmoid region\n","            For instance, threshold_init_cutoff = 0.9 will set 10% points equal to 0.0 or 1.0\n","            Setting this value > 1.0 will result in a margin between data points and sparse-sigmoid cutoff value\n","            All points will be between (0.5 - 0.5 / threshold_init_cutoff) and (0.5 + 0.5 / threshold_init_cutoff)\n","        \"\"\"\n","        super().__init__()\n","        self.depth, self.num_trees, self.tree_dim, self.flatten_output = depth, num_trees, tree_dim, flatten_output\n","        self.choice_function, self.bin_function = choice_function, bin_function\n","        self.threshold_init_beta, self.threshold_init_cutoff = threshold_init_beta, threshold_init_cutoff\n","\n","        self.response = nn.Parameter(torch.zeros([num_trees, tree_dim, 2 ** depth]), requires_grad=True)\n","        initialize_response_(self.response)\n","\n","        self.feature_selection_logits = nn.Parameter(\n","            torch.zeros([in_features, num_trees, depth]), requires_grad=True\n","        )\n","        initialize_selection_logits_(self.feature_selection_logits)\n","\n","        self.feature_thresholds = nn.Parameter(\n","            torch.full([num_trees, depth], float('nan'), dtype=torch.float32), requires_grad=True\n","        )  # nan values will be initialized on first batch (data-aware init)\n","\n","        self.log_temperatures = nn.Parameter(\n","            torch.full([num_trees, depth], float('nan'), dtype=torch.float32), requires_grad=True\n","        )\n","\n","        # binary codes for mapping between 1-hot vectors and bin indices\n","        with torch.no_grad():\n","            indices = torch.arange(2 ** self.depth)\n","            offsets = 2 ** torch.arange(self.depth)\n","            bin_codes = (indices.view(1, -1) // offsets.view(-1, 1) % 2).to(torch.float32)\n","            bin_codes_1hot = torch.stack([bin_codes, 1.0 - bin_codes], dim=-1)\n","            self.bin_codes_1hot = nn.Parameter(bin_codes_1hot, requires_grad=False)\n","            # ^-- [depth, 2 ** depth, 2]\n","\n","    def forward(self, input):\n","        assert len(input.shape) >= 2\n","        if len(input.shape) > 2:\n","            return self.forward(input.view(-1, input.shape[-1])).view(*input.shape[:-1], -1)\n","        # new input shape: [batch_size, in_features]\n","\n","        feature_logits = self.feature_selection_logits\n","        feature_selectors = self.choice_function(feature_logits, dim=0)\n","        # ^--[in_features, num_trees, depth]\n","\n","        feature_values = torch.einsum('bi,ind->bnd', input, feature_selectors)\n","        # ^--[batch_size, num_trees, depth]\n","\n","        threshold_logits = (feature_values - self.feature_thresholds) * torch.exp(-self.log_temperatures)\n","\n","        threshold_logits = torch.stack([-threshold_logits, threshold_logits], dim=-1)\n","        # ^--[batch_size, num_trees, depth, 2]\n","\n","        bins = self.bin_function(threshold_logits)\n","        # ^--[batch_size, num_trees, depth, 2], approximately binary\n","\n","        bin_matches = torch.einsum('btds,dcs->btdc', bins, self.bin_codes_1hot)\n","        # ^--[batch_size, num_trees, depth, 2 ** depth]\n","\n","        response_weights = torch.prod(bin_matches, dim=-2)\n","        # ^-- [batch_size, num_trees, 2 ** depth]\n","\n","        response = torch.einsum('bnd,ncd->bnc', response_weights, self.response)\n","        # ^-- [batch_size, num_trees, tree_dim]\n","\n","        return response.flatten(1, 2) if self.flatten_output else response\n","\n","    def initialize(self, input, eps=1e-6):\n","        # data-aware initializer\n","        assert len(input.shape) == 2\n","        if input.shape[0] < 1000:\n","            warn(\"Data-aware initialization is performed on less than 1000 data points. This may cause instability.\"\n","                 \"To avoid potential problems, run this model on a data batch with at least 1000 data samples.\"\n","                 \"You can do so manually before training. Use with torch.no_grad() for memory efficiency.\")\n","        with torch.no_grad():\n","            feature_selectors = self.choice_function(self.feature_selection_logits, dim=0)\n","            # ^--[in_features, num_trees, depth]\n","\n","            feature_values = torch.einsum('bi,ind->bnd', input, feature_selectors)\n","            # ^--[batch_size, num_trees, depth]\n","\n","            # initialize thresholds: sample random percentiles of data\n","            percentiles_q = 100 * np.random.beta(self.threshold_init_beta, self.threshold_init_beta,\n","                                                 size=[self.num_trees, self.depth])\n","            self.feature_thresholds.data[...] = torch.as_tensor(\n","                list(map(np.percentile, check_numpy(feature_values.flatten(1, 2).t()), percentiles_q.flatten())),\n","                dtype=feature_values.dtype, device=feature_values.device\n","            ).view(self.num_trees, self.depth)\n","\n","            # init temperatures: make sure enough data points are in the linear region of sparse-sigmoid\n","            temperatures = np.percentile(check_numpy(abs(feature_values - self.feature_thresholds)),\n","                                         q=100 * min(1.0, self.threshold_init_cutoff), axis=0)\n","\n","            # if threshold_init_cutoff > 1, scale everything down by it\n","            temperatures /= max(1.0, self.threshold_init_cutoff)\n","            self.log_temperatures.data[...] = torch.log(torch.as_tensor(temperatures) + eps)\n","\n","    def __repr__(self):\n","        return \"{}(in_features={}, num_trees={}, depth={}, tree_dim={}, flatten_output={})\".format(\n","            self.__class__.__name__, self.feature_selection_logits.shape[0],\n","            self.num_trees, self.depth, self.tree_dim, self.flatten_output\n","        )\n","    \n","class DenseBlock(nn.Sequential):\n","    def __init__(self, input_dim, layer_dim, num_layers, tree_dim=1, max_features=None,\n","                 input_dropout=0.0, flatten_output=True, Module=ODST, **kwargs):\n","        layers = []\n","        for i in range(num_layers):\n","            oddt = Module(input_dim, layer_dim, tree_dim=tree_dim, flatten_output=True, **kwargs)\n","            input_dim = min(input_dim + layer_dim * tree_dim, max_features or float('inf'))\n","            layers.append(oddt)\n","\n","        super().__init__(*layers)\n","        self.num_layers, self.layer_dim, self.tree_dim = num_layers, layer_dim, tree_dim\n","        self.max_features, self.flatten_output = max_features, flatten_output\n","        self.input_dropout = input_dropout\n","\n","    def forward(self, x):\n","        initial_features = x.shape[-1]\n","        for layer in self:\n","            layer_inp = x\n","            if self.max_features is not None:\n","                tail_features = min(self.max_features, layer_inp.shape[-1]) - initial_features\n","                if tail_features != 0:\n","                    layer_inp = torch.cat([layer_inp[..., :initial_features], layer_inp[..., -tail_features:]], dim=-1)\n","            if self.training and self.input_dropout:\n","                layer_inp = F.dropout(layer_inp, self.input_dropout)\n","            h = layer(layer_inp)\n","            x = torch.cat([x, h], dim=-1)\n","\n","        outputs = x[..., initial_features:]\n","        if not self.flatten_output:\n","            outputs = outputs.view(*outputs.shape[:-1], self.num_layers * self.layer_dim, self.tree_dim)\n","        return outputs"],"execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# QHAdam\n","\n","https://github.com/facebookresearch/qhoptim"]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":["from torch.optim.optimizer import Optimizer, required\n","import collections\n","import math\n","\n","QHMParams = collections.namedtuple(\"QHMParams\", [\"alpha\", \"nu\", \"beta\"])\n","\n","QHAdamParams = collections.namedtuple(\"QHAdamParams\", [\"alpha\", \"nu1\", \"nu2\", \"beta1\", \"beta2\"])\n","\n","\n","def from_pid(k_p, k_i, k_d):\n","    alpha = k_i\n","    nu = k_p * k_p / (k_i * k_d)\n","    beta = k_d / (k_d - k_p)\n","    return QHMParams(alpha=alpha, nu=nu, beta=beta)\n","\n","\n","def from_synthesized_nesterov(alpha, beta1, beta2):\n","    new_alpha = alpha / (1.0 - beta1)\n","    nu = 1.0 - ((1.0 - beta1) / beta1) * beta2\n","    beta = beta1\n","    return QHMParams(alpha=new_alpha, nu=nu, beta=beta)\n","\n","\n","def from_robust_momentum(l, kappa, rho):\n","    if rho is None:\n","        rho = 1.0 - 1.0 / math.sqrt(kappa)\n","\n","    alpha = kappa * ((1.0 - rho) ** 2) * (1.0 + rho) / l\n","    beta1 = kappa * (rho ** 3) / (kappa - 1.0)\n","    beta2 = (rho ** 3) / ((kappa - 1.0) * ((1.0 - rho) ** 2) * (1.0 + rho))\n","    return from_synthesized_nesterov(alpha, beta1, beta2)\n","\n","\n","def from_accsgd(delta, kappa, xi, eps):\n","    alpha = (delta * eps * (1.0 + xi)) / (1.0 + eps)\n","    nu = (eps * xi - 1.0) / (eps * (1.0 + xi))\n","    beta = (kappa - (eps * eps) * xi) / (kappa + eps * xi)\n","    return QHMParams(alpha=alpha, nu=nu, beta=beta)\n","\n","\n","def from_two_state_optimizer(h, k, l, m, q, z):\n","    phi = math.sqrt((h - q) * (h - q) + 4.0 * k * m)\n","    psi = k * m - h * q\n","    xi = (h - q - phi) * (l * m - h * z) + 2.0 * m * (l * q - k * z)\n","\n","    alpha = 0.5 * xi / (phi * psi)\n","    nu = 2.0 * m * (l * q - k * z) / xi\n","    beta = 0.5 * (h + q - phi)\n","    return QHMParams(alpha=alpha, nu=nu, beta=beta)\n","\n","\n","def from_nadam(lr, beta1, beta2):\n","    return QHAdamParams(alpha=lr, nu1=beta1, nu2=1.0, beta1=beta1, beta2=beta2)\n","\n","class QHM(Optimizer):\n","    r\"\"\"Implements the quasi-hyperbolic momentum (QHM) optimization algorithm\n","    `(Ma and Yarats, 2019)`_.\n","    Note that many other optimization algorithms are accessible via specific\n","    parameterizations of QHM. See :func:`from_accsgd()`,\n","    :func:`from_robust_momentum()`, etc. for details.\n","    Args:\n","        params (iterable):\n","            iterable of parameters to optimize or dicts defining parameter\n","            groups\n","        lr (float):\n","            learning rate (:math:`\\alpha` from the paper)\n","        momentum (float):\n","            momentum factor (:math:`\\beta` from the paper)\n","        nu (float):\n","            immediate discount factor (:math:`\\nu` from the paper)\n","        weight_decay (float, optional):\n","            weight decay (L2 regularization coefficient, times two)\n","            (default: 0.0)\n","        weight_decay_type (str, optional):\n","            method of applying the weight decay:\n","            ``\"grad\"`` for accumulation in the gradient\n","            (same as :class:`torch.optim.SGD`) or\n","            ``\"direct\"`` for direct application to the parameters\n","            (default: ``\"grad\"``)\n","    Example:\n","        >>> optimizer = qhoptim.pyt.QHM(\n","        ...     model.parameters(), lr=1.0, nu=0.7, momentum=0.999)\n","        >>> optimizer.zero_grad()\n","        >>> loss_fn(model(input), target).backward()\n","        >>> optimizer.step()\n","    .. _`(Ma and Yarats, 2019)`: https://arxiv.org/abs/1810.06801\n","    .. note::\n","        Mathematically, QHM is a simple interpolation between plain SGD and\n","        momentum:\n","        .. math::\n","            \\begin{align*}\n","                g_{t + 1} &\\leftarrow\n","                    \\beta \\cdot g_t +\n","                    (1 - \\beta) \\cdot \\nabla_t \\\\\n","                \\theta_{t + 1} &\\leftarrow\n","                    \\theta_t + \\alpha \\left[ (1 - \\nu) \\cdot \\nabla_t +\n","                                             \\nu \\cdot g_{t + 1} \\right]\n","            \\end{align*}\n","        Here, :math:`\\alpha` is the learning rate, :math:`\\beta` is the momentum\n","        factor, and :math:`\\nu` is the \"immediate discount\" factor which\n","        controls the interpolation between plain SGD and momentum.\n","        :math:`g_t` is the momentum buffer, :math:`\\theta_t` is the parameter\n","        vector, and :math:`\\nabla_t` is the gradient with respect to\n","        :math:`\\theta_t`.\n","    .. note::\n","        QHM uses **dampened** momentum. This means that when converting from\n","        plain momentum to QHM, the learning rate must be scaled by\n","        :math:`\\frac{1}{1 - \\beta}`. For example, momentum with learning rate\n","        :math:`\\alpha = 0.1` and momentum :math:`\\beta = 0.9` should be\n","        converted to QHM with learning rate :math:`\\alpha = 1.0`.\n","    \"\"\"\n","\n","    def __init__(self, params, lr=required, momentum=required, nu=required, weight_decay=0.0, weight_decay_type=\"grad\"):\n","        if lr is not required and lr < 0.0:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if momentum < 0.0:\n","            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n","        if weight_decay < 0.0:\n","            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n","        if weight_decay_type not in (\"grad\", \"direct\"):\n","            raise ValueError(\"Invalid weight_decay_type value: {}\".format(weight_decay_type))\n","\n","        defaults = {\n","            \"lr\": lr,\n","            \"momentum\": momentum,\n","            \"nu\": nu,\n","            \"weight_decay\": weight_decay,\n","            \"weight_decay_type\": weight_decay_type,\n","        }\n","        super(QHM, self).__init__(params, defaults)\n","\n","    def step(self, closure=None):\n","        \"\"\"Performs a single optimization step.\n","        Args:\n","            closure (callable, optional):\n","                A closure that reevaluates the model and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            lr, nu, momentum = group[\"lr\"], group[\"nu\"], group[\"momentum\"]\n","            weight_decay, weight_decay_type = group[\"weight_decay\"], group[\"weight_decay_type\"]\n","\n","            for p in group[\"params\"]:\n","                if p.grad is None:\n","                    continue\n","                d_p = p.grad.data\n","                param_state = self.state[p]\n","\n","                if weight_decay != 0:\n","                    if weight_decay_type == \"grad\":\n","                        d_p.add_(weight_decay, p.data)\n","                    elif weight_decay_type == \"direct\":\n","                        p.data.mul_(1.0 - lr * weight_decay)\n","                    else:\n","                        raise ValueError(\"Invalid weight decay type provided\")\n","\n","                if len(param_state) == 0:\n","                    param_state[\"momentum_buffer\"] = torch.zeros_like(p.data)\n","\n","                momentum_buffer = param_state[\"momentum_buffer\"]\n","                momentum_buffer.mul_(momentum).add_(1.0 - momentum, d_p)\n","\n","                p.data.add_(-lr * nu, momentum_buffer)\n","                p.data.add_(-lr * (1.0 - nu), d_p)\n","\n","        return loss\n","\n","    @classmethod\n","    def _params_to_dict(cls, params):\n","        return {\"lr\": params.alpha, \"nu\": params.nu, \"momentum\": params.beta}\n","\n","    @classmethod\n","    def from_pid(cls, k_p, k_i, k_d):\n","        r\"\"\"Calculates the QHM hyperparameters required to recover a PID\n","        optimizer as described in `Recht (2018)`_.\n","        Args:\n","            k_p (float):\n","                proportional gain (see reference)\n","            k_i (float):\n","                integral gain (see reference)\n","            k_d (float):\n","                derivative gain (see reference)\n","        Returns:\n","            Three-element ``dict`` containing ``lr``, ``momentum``, and ``nu``\n","            to use in QHM.\n","        Example:\n","            >>> optimizer = qhoptim.pyt.QHM(\n","            ...     model.parameters(),\n","            ...     weight_decay=1e-4,\n","            ...     **qhoptim.pyt.QHM.from_pid(\n","            ...         k_p=-0.1, k_i=1.0, k_d=3.0))\n","        .. _`Recht (2018)`: https://web.archive.org/web/20181027184056/http://www.argmin.net/2018/04/19/pid/\n","        \"\"\"\n","        return cls._params_to_dict(param_conv.from_pid(k_p, k_i, k_d))\n","\n","    @classmethod\n","    def from_synthesized_nesterov(cls, alpha, beta1, beta2):\n","        r\"\"\"Calculates the QHM hyperparameters required to recover the\n","        synthesized Nesterov optimizer (Section 6 of `Lessard et al. (2016)`_).\n","        Args:\n","            alpha (float):\n","                learning rate\n","            beta1 (float):\n","                first momentum (see reference)\n","            beta2 (float):\n","                second momentum (see reference)\n","        Returns:\n","            Three-element ``dict`` containing ``lr``, ``momentum``, and ``nu``\n","            to use in QHM.\n","        Example:\n","            >>> optimizer = qhoptim.pyt.QHM(\n","            ...     model.parameters(),\n","            ...     weight_decay=1e-4,\n","            ...     **qhoptim.pyt.QHM.from_synthesized_nesterov(\n","            ...         alpha=0.1, beta1=0.9, beta2=0.6))\n","        .. _`Lessard et al. (2016)`: https://arxiv.org/abs/1408.3595\n","        \"\"\"\n","        return cls._params_to_dict(param_conv.from_synthesized_nesterov(alpha, beta1, beta2))\n","\n","    @classmethod\n","    def from_robust_momentum(cls, l, kappa, rho=None):\n","        r\"\"\"Calculates the QHM hyperparameters required to recover the Robust\n","        Momentum `(Cyrus et al., 2018)`_ or Triple Momentum\n","        `(Scoy et al., 2018)`_ optimizers.\n","        Args:\n","            l (float):\n","                Lipschitz constant of gradient (see reference)\n","            kappa (float):\n","                condition ratio (see reference)\n","            rho (float, optional):\n","                noise-free convergence rate. If None, will return the\n","                parameters for the Triple Momentum optimizer.\n","        Returns:\n","            Three-element ``dict`` containing ``lr``, ``momentum``, and ``nu``\n","            to use in QHM.\n","        Example:\n","            >>> optimizer = qhoptim.pyt.QHM(\n","            ...     model.parameters(),\n","            ...     weight_decay=1e-4,\n","            ...     **qhoptim.pyt.QHM.from_robust_momentum(\n","            ...         l=5.0, kappa=15.0))\n","        .. _`(Cyrus et al., 2018)`: https://arxiv.org/abs/1710.04753\n","        .. _`(Scoy et al., 2018)`: http://www.optimization-online.org/DB_FILE/2017/03/5908.pdf\n","        \"\"\"\n","        return cls._params_to_dict(param_conv.from_robust_momentum(l, kappa, rho))\n","\n","    @classmethod\n","    def from_accsgd(cls, delta, kappa, xi, eps=0.7):\n","        r\"\"\"Calculates the QHM hyperparameters required to recover the AccSGD\n","        optimizer `(Kidambi et al., 2018)`_.\n","        Args:\n","            delta (float):\n","                short step (see reference)\n","            kappa (float):\n","                long step parameter (see reference)\n","            xi (float):\n","                statistical advantage parameter (see reference)\n","            eps (float, optional):\n","                arbitrary value, between 0 and 1 exclusive (see reference)\n","                (default: 0.7)\n","        Returns:\n","            Three-element ``dict`` containing ``lr``, ``momentum``, and ``nu``\n","            to use in QHM.\n","        Example:\n","            >>> optimizer = qhoptim.pyt.QHM(\n","            ...     model.parameters(),\n","            ...     weight_decay=1e-4,\n","            ...     **qhoptim.pyt.QHM.from_accsgd(\n","            ...         delta=0.1, kappa=1000.0, xi=10.0))\n","        .. _`(Kidambi et al., 2018)`: https://arxiv.org/abs/1803.05591\n","        \"\"\"\n","        return cls._params_to_dict(param_conv.from_accsgd(delta, kappa, xi, eps))\n","\n","    @classmethod\n","    def from_two_state_optimizer(cls, h, k, l, m, q, z):\n","        r\"\"\"Calculates the QHM hyperparameters required to recover the\n","        following optimizer (named \"TSO\" in `Ma and Yarats (2019)`_):\n","        .. math::\n","            \\begin{align*}\n","                a_{t + 1} &\\leftarrow\n","                    h \\cdot a_t + k \\cdot \\theta_t + l \\cdot \\nabla_t \\\\\n","                \\theta_{t + 1} &\\leftarrow\n","                    m \\cdot a_t + q \\cdot \\theta_t + z \\cdot \\nabla_t\n","            \\end{align*}\n","        Here, :math:`a_t` and :math:`\\theta_t` are the two states and\n","        :math:`\\nabla_t` is the gradient with respect to :math:`\\theta_t`.\n","        Be careful that your coefficients satisfy the regularity conditions\n","        from the reference.\n","        Args:\n","            h (float):\n","                see description\n","            k (float):\n","                see description\n","            l (float):\n","                see description\n","            m (float):\n","                see description\n","            q (float):\n","                see description\n","            z (float):\n","                see description\n","        Returns:\n","            Three-element ``dict`` containing ``lr``, ``momentum``, and ``nu``\n","            to use in QHM.\n","        Example:\n","            >>> optimizer = qhoptim.pyt.QHM(\n","            ...     model.parameters(),\n","            ...     weight_decay=1e-4,\n","            ...     **qhoptim.pyt.QHM.from_two_state_optimizer(\n","            ...         h=0.9, k=0.0, l=0.1, m=-0.09, q=1.0, z=-0.01))\n","        .. _`Ma and Yarats (2019)`: https://arxiv.org/abs/1810.06801\n","        \"\"\"\n","        return cls._params_to_dict(param_conv.from_two_state_optimizer(h, k, l, m, q, z))\n","\n","class QHAdam(Optimizer):\n","    r\"\"\"Implements the QHAdam optimization algorithm `(Ma and Yarats, 2019)`_.\n","    Note that the NAdam optimizer is accessible via a specific parameterization\n","    of QHAdam. See :func:`from_nadam()` for details.\n","    Args:\n","        params (iterable):\n","            iterable of parameters to optimize or dicts defining parameter\n","            groups\n","        lr (float, optional): learning rate (:math:`\\alpha` from the paper)\n","            (default: 1e-3)\n","        betas (Tuple[float, float], optional): coefficients used for computing\n","            running averages of the gradient and its square\n","            (default: (0.9, 0.999))\n","        nus (Tuple[float, float], optional): immediate discount factors used to\n","            estimate the gradient and its square\n","            (default: (1.0, 1.0))\n","        eps (float, optional): term added to the denominator to improve\n","            numerical stability\n","            (default: 1e-8)\n","        weight_decay (float, optional): weight decay (default: 0.0)\n","        decouple_weight_decay (bool, optional): whether to decouple the weight\n","            decay from the gradient-based optimization step\n","            (default: False)\n","    Example:\n","        >>> optimizer = qhoptim.pyt.QHAdam(\n","        ...     model.parameters(),\n","        ...     lr=3e-4, nus=(0.8, 1.0), betas=(0.99, 0.999))\n","        >>> optimizer.zero_grad()\n","        >>> loss_fn(model(input), target).backward()\n","        >>> optimizer.step()\n","    .. _`(Ma and Yarats, 2019)`: https://arxiv.org/abs/1810.06801\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        params,\n","        lr=1e-3,\n","        betas=(0.9, 0.999),\n","        nus=(1.0, 1.0),\n","        weight_decay=0.0,\n","        decouple_weight_decay=False,\n","        eps=1e-8,\n","    ):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        if not 0.0 <= betas[0] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n","        if not 0.0 <= betas[1] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n","        if weight_decay < 0.0:\n","            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n","\n","        defaults = {\n","            \"lr\": lr,\n","            \"betas\": betas,\n","            \"nus\": nus,\n","            \"weight_decay\": weight_decay,\n","            \"decouple_weight_decay\": decouple_weight_decay,\n","            \"eps\": eps,\n","        }\n","        super(QHAdam, self).__init__(params, defaults)\n","\n","    def step(self, closure=None):\n","        \"\"\"Performs a single optimization step.\n","        Args:\n","            closure (callable, optional):\n","                A closure that reevaluates the model and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            lr = group[\"lr\"]\n","            beta1, beta2 = group[\"betas\"]\n","            nu1, nu2 = group[\"nus\"]\n","            weight_decay = group[\"weight_decay\"]\n","            decouple_weight_decay = group[\"decouple_weight_decay\"]\n","            eps = group[\"eps\"]\n","\n","            for p in group[\"params\"]:\n","                if p.grad is None:\n","                    continue\n","\n","                d_p = p.grad.data\n","                if d_p.is_sparse:\n","                    raise RuntimeError(\"QHAdam does not support sparse gradients\")\n","\n","                param_state = self.state[p]\n","\n","                if weight_decay != 0:\n","                    if decouple_weight_decay:\n","                        p.data.mul_(1 - lr * weight_decay)\n","                    else:\n","                        d_p.add_(weight_decay, p.data)\n","\n","                d_p_sq = d_p.mul(d_p)\n","\n","                if len(param_state) == 0:\n","                    param_state[\"beta1_weight\"] = 0.0\n","                    param_state[\"beta2_weight\"] = 0.0\n","                    param_state[\"exp_avg\"] = torch.zeros_like(p.data)\n","                    param_state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n","\n","                param_state[\"beta1_weight\"] = 1.0 + beta1 * param_state[\"beta1_weight\"]\n","                param_state[\"beta2_weight\"] = 1.0 + beta2 * param_state[\"beta2_weight\"]\n","\n","                beta1_weight = param_state[\"beta1_weight\"]\n","                beta2_weight = param_state[\"beta2_weight\"]\n","                exp_avg = param_state[\"exp_avg\"]\n","                exp_avg_sq = param_state[\"exp_avg_sq\"]\n","\n","                beta1_adj = 1.0 - (1.0 / beta1_weight)\n","                beta2_adj = 1.0 - (1.0 / beta2_weight)\n","                exp_avg.mul_(beta1_adj).add_(1.0 - beta1_adj, d_p)\n","                exp_avg_sq.mul_(beta2_adj).add_(1.0 - beta2_adj, d_p_sq)\n","\n","                avg_grad = exp_avg.mul(nu1)\n","                if nu1 != 1.0:\n","                    avg_grad.add_(1.0 - nu1, d_p)\n","\n","                avg_grad_rms = exp_avg_sq.mul(nu2)\n","                if nu2 != 1.0:\n","                    avg_grad_rms.add_(1.0 - nu2, d_p_sq)\n","                avg_grad_rms.sqrt_()\n","                if eps != 0.0:\n","                    avg_grad_rms.add_(eps)\n","\n","                p.data.addcdiv_(-lr, avg_grad, avg_grad_rms)\n","\n","        return loss\n","\n","    @classmethod\n","    def _params_to_dict(cls, params):\n","        return {\"lr\": params.alpha, \"nus\": (params.nu1, params.nu2), \"betas\": (params.beta1, params.beta2)}\n","\n","    @classmethod\n","    def from_nadam(cls, lr=1e-3, betas=(0.9, 0.999)):\n","        r\"\"\"Calculates the QHAdam hyperparameters required to recover the NAdam\n","        optimizer `(Dozat, 2016)`_.\n","        This is *not* an identical recovery of the formulation in the paper, due\n","        to subtle differences in the application of the bias correction in the\n","        first moment estimator. However, in practice, this difference is almost\n","        certainly irrelevant.\n","        Args:\n","            lr (float, optional):\n","                learning rate (:math:`\\alpha` from the paper)\n","                (default: 1e-3)\n","            betas (Tuple[float, float], optional):\n","                coefficients used for computing running averages of the\n","                gradient and its square\n","                (default: (0.9, 0.999))\n","        Returns:\n","            Three-element ``dict`` containing ``lr``, ``betas``, and ``nus``\n","            to use in QHAdam.\n","        Example:\n","            >>> optimizer = qhoptim.pyt.QHAdam(\n","            ...     model.parameters(),\n","            ...     weight_decay=1e-4,\n","            ...     **qhoptim.pyt.QHAdam.from_nadam(\n","            ...         lr=1e-3, betas=(0.9, 0.999)))\n","        .. _`(Dozat, 2016)`: https://openreview.net/pdf?id=OM0jvwB8jIp57ZJjtNEZ\n","        \"\"\"\n","        return cls._params_to_dict(param_conv.from_nadam(lr, betas[0], betas[1]))\n","\n","\n","def QHAdamW(params, *args, **kwargs):\n","    r\"\"\"Constructs the decoupled decay variant of the QHAdam optimization\n","    algorithm `(Ma and Yarats, 2019)`_,\n","    as proposed by `Loschilov and Hutter (2017)`_.\n","    Shares all arguments of the :class:`QHAdam` constructor â€“\n","    equivalent to constructing :class:`QHAdam` with\n","    ``decouple_weight_decay=True``.\n","    .. _`Loschilov and Hutter (2017)`: https://arxiv.org/abs/1711.05101\n","    .. _`(Ma and Yarats, 2019)`: https://arxiv.org/abs/1810.06801\n","    \"\"\"\n","    return QHAdam(params, *args, decouple_weight_decay=True, **kwargs)"],"execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Features"]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":["top_feats = [  1,   2,   3,   4,   5,   6,   7,   9,  11,  14,  15,  16,  17,\n","        18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n","        32,  33,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  46,\n","        47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,  60,\n","        61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n","        74,  75,  76,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n","        89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n","       102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n","       115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n","       129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143,\n","       144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n","       158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n","       171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n","       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197,\n","       198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212,\n","       213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226,\n","       227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n","       240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n","       254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n","       267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n","       281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294,\n","       295, 296, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n","       310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n","       324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n","       337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n","       350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362,\n","       363, 364, 365, 366, 367, 368, 369, 370, 371, 374, 375, 376, 377,\n","       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391,\n","       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n","       405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418,\n","       419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n","       432, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446,\n","       447, 448, 449, 450, 453, 454, 456, 457, 458, 459, 460, 461, 462,\n","       463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n","       476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n","       490, 491, 492, 493, 494, 495, 496, 498, 500, 501, 502, 503, 505,\n","       506, 507, 509, 510, 511, 512, 513, 514, 515, 518, 519, 520, 521,\n","       522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 534, 535, 536,\n","       538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n","       552, 554, 557, 559, 560, 561, 562, 565, 566, 567, 568, 569, 570,\n","       571, 572, 573, 574, 575, 577, 578, 580, 581, 582, 583, 584, 585,\n","       586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599,\n","       600, 601, 602, 606, 607, 608, 609, 611, 612, 613, 615, 616, 617,\n","       618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n","       631, 632, 633, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644,\n","       645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 658, 659,\n","       660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n","       673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n","       686, 687, 688, 689, 691, 692, 693, 694, 695, 696, 697, 699, 700,\n","       701, 702, 704, 705, 707, 708, 709, 710, 711, 713, 714, 716, 717,\n","       718, 720, 721, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n","       733, 734, 735, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747,\n","       748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761,\n","       762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774,\n","       775, 776, 777, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n","       789, 790, 792, 793, 794, 795, 796, 797, 798, 800, 801, 802, 803,\n","       804, 805, 806, 808, 809, 811, 813, 814, 815, 816, 817, 818, 819,\n","       821, 822, 823, 825, 826, 827, 828, 829, 830, 831, 832, 834, 835,\n","       837, 838, 839, 840, 841, 842, 845, 846, 847, 848, 850, 851, 852,\n","       854, 855, 856, 858, 859, 860, 861, 862, 864, 866, 867, 868, 869,\n","       870, 871, 872, 873, 874]\n","\n","print(len(top_feats))"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["785\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":["# dataset class\n","class MoaDataset(Dataset):\n","    def __init__(self, df, targets, feats_idx, mode='train'):\n","        self.mode = mode\n","        self.feats = feats_idx\n","        self.data = df[:, feats_idx]\n","        if mode=='train':\n","            self.targets = targets\n","    \n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, idx):\n","        if self.mode == 'train':\n","            return torch.FloatTensor(self.data[idx]), torch.FloatTensor(self.targets[idx])\n","        elif self.mode == 'test':\n","            return torch.FloatTensor(self.data[idx]), 0"],"execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["train = train.values\n","test = test.values\n","train_targets = train_targets.values"],"execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def mean_log_loss(y_true, y_pred):\n","    metrics = []\n","    for i, target in enumerate(targets):\n","        metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float), labels=[0,1]))\n","    return np.mean(metrics)"],"execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Inference"]},{"metadata":{"trusted":true},"cell_type":"code","source":["phase = \"test\"\n","kfold = MultilabelStratifiedKFold(n_splits=nfolds, random_state=seed, shuffle=True)\n","for seed in range(nstarts):\n","    print(f\"Inference for seed {seed}\")\n","    seed_targets = []\n","    seed_oof = []\n","    seed_preds = np.zeros((len(test), ntargets, nfolds))\n","    \n","    for n, (tr, te) in enumerate(kfold.split(train_targets, train_targets)):\n","        xval, yval = train[te], train_targets[te]\n","        fold_preds = []\n","        test_set = MoaDataset(test, None, top_feats, mode='test')\n","        \n","        dataloaders = {\n","            'test': DataLoader(test_set, batch_size=val_batch_size, shuffle=False)\n","        }\n","        \n","        checkpoint_path = f'Model_{seed}_Fold_{n+1}.pt'\n","        model = nn.Sequential(nn.BatchNorm1d(len(top_feats)), \n","                              DenseBlock(len(top_feats), layer_dim = 16, num_layers = 4, tree_dim = 512, depth = 6, input_dropout = 0.3,\n","                                         flatten_output = True, choice_function = entmax15, bin_function = entmoid15), \n","                              nn.BatchNorm1d(16 * 4 * 512), \n","                              nn.Dropout(0.5), \n","                              nn.utils.weight_norm(nn.Linear(16 * 4 * 512, 206)), \n","                              nn.Sigmoid(), \n","                             ).to(device)\n","        model.load_state_dict(torch.load(checkpoint_path))\n","        model.eval()\n","        for i, (x, y) in enumerate(dataloaders[phase]):\n","            x = x.to(device)\n","            with torch.no_grad():\n","                batch_preds = model(x)\n","                fold_preds.append(batch_preds)\n","        fold_preds = torch.cat(fold_preds, dim=0).cpu().numpy()\n","        seed_preds[:, :, n] = fold_preds\n","\n","    seed_preds = np.mean(seed_preds, axis=2)\n","    preds += seed_preds / nstarts"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Inference for seed 0\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'x' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-23bfdf1d20ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mbatch_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":["ss[targets] = preds\n","ss.loc[test_features['cp_type']=='ctl_vehicle', targets] = 0\n","ss.to_csv('submission.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"name":"Python 3.6.5 64-bit ('venv')","display_name":"Python 3.6.5 64-bit ('venv')","metadata":{"interpreter":{"hash":"ece7588252db02ccc8b5c5093cb786e4144f947b34c09a84a022e60d577a0d4f"}}},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.5-final","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}