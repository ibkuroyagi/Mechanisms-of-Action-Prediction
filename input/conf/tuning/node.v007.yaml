# to check QHAdam's stability
# add preprocess
# loss: SmoothBCEwLogits 0.0005
###########################################################
#                      GLOBAL SETTING                     #
###########################################################
seed: 1
###########################################################
#                    PREPROCESS SETTING                   #
###########################################################
QuantileTransformer:
  n_quantiles: 100
  random_state: 1
  output_distribution: normal
n_comp_g: 600
n_comp_c: 35
VarianceThreshold: 0
n_cluster_g: 35
n_cluster_c: 10
###########################################################
#                     NETWORK SETTING                     #
###########################################################
model_params:
  layer_dim: 16
  num_layers: 4
  tree_dim: 1
  depth: 6
  input_dropout: 0.3
  flatten_output: True
out_dim: 206
###########################################################
#                    TRAINING SETTING                     #
###########################################################
n_fold: 10
batch_size: 128
num_workers: 2
pin_memory: True
accum_grads: 1
###########################################################
#                       LOSS SETTING                      #
###########################################################
loss_type: SmoothBCEwLogits
loss_params:
  reduction: sum
  smoothing: 0.0005

###########################################################
#              OPTIMIZER & SCHEDULER SETTING              #
###########################################################
optimizer_type: QHAdam
optimizer_params:
  lr: 1.0e-3
  nus: [0.7, 1.0]
  betas: [0.995, 0.999]
  weight_decay: 1.0e-5

scheduler_type: ReduceLROnPlateau
scheduler_params:
  mode: min
  factor: 0.2
  patience: 10
  verbose: False
  
###########################################################
#                    INTERVAL SETTING                     #
###########################################################
train_max_steps: 40000 # Number of training steps.
save_interval_steps: 10000 # Interval steps to save checkpoint.
